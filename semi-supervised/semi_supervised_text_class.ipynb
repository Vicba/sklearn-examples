{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semi-supervised Classification on a Text Dataset#\n",
        "\n",
        "In this example, semi-supervised classifiers are trained on the 20 newsgroups dataset (which will be automatically downloaded).\n",
        "\n",
        "You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get all 20 of them."
      ],
      "metadata": {
        "id": "fCC5ZX6CCr8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Semi-Supervised Learning?\n",
        "\n",
        "Semi-supervised learning is a machine learning technique that uses a combination of labeled and unlabeled data to train a model. It bridges the gap between supervised learning, which requires labeled data, and unsupervised learning, which uses only unlabeled data. The key idea is to leverage the large amounts of unlabeled data available to improve model performance, especially when labeled data is scarce or expensive to obtain[1][2][3].\n",
        "\n",
        "## When to Use Semi-Supervised Learning\n",
        "\n",
        "Semi-supervised learning is particularly useful in the following scenarios:\n",
        "\n",
        "- **Limited resources for labeling data**, such as medical images that require annotation by specialists[1]\n",
        "- **Large volumes of unlabeled data available**, like in social networks or the internet, which can be leveraged to improve models[1]\n",
        "- **Working with unstructured data** like text, images, or audio that is difficult to label[1]\n",
        "- **Dealing with rare classes** in classification tasks, where labeled examples may be limited, such as in fraud detection or rare disease diagnosis[1][2]\n",
        "- **When the labeled data alone is not representative of the entire data distribution**, making supervised learning ineffective[3]\n",
        "\n",
        "However, semi-supervised learning may not be suitable if the labeled data is not representative of the underlying data distribution. It is most effective when there is a significant amount of unlabeled data available and the data follows certain assumptions like continuity, clustering, or manifold structure."
      ],
      "metadata": {
        "id": "CttgywZYGeCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDTgPsPYCkJ1",
        "outputId": "125de505-6255-49bb-858e-26564487aca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2823 documents\n",
            "5 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
        "\n",
        "# Loading dataset containing first five categories\n",
        "data = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    categories=[\n",
        "        \"alt.atheism\",\n",
        "        \"comp.graphics\",\n",
        "        \"comp.os.ms-windows.misc\",\n",
        "        \"comp.sys.ibm.pc.hardware\",\n",
        "        \"comp.sys.mac.hardware\",\n",
        "    ],\n",
        ")\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "sdg_params, vectorizer_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CTPGo8bC6wD",
        "outputId": "b9920c81-200e-4bae-aac9-28f255441710"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'alpha': 1e-05, 'penalty': 'l2', 'loss': 'log_loss'},\n",
              " {'ngram_range': (1, 2), 'min_df': 5, 'max_df': 0.8})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"hoi this is me\", \"I'm august is me\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI78Zfr4EkYw",
        "outputId": "e0941d08-edfa-4813-93dd-97e1ef07521b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['august' 'hoi' 'is' 'me' 'this']\n",
            "[[0 1 1 1 1]\n",
            " [1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# supervised pipeline => trained exlusively on labeled data\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        # Step 1: Convert the text documents into a bag-of-words model\n",
        "        # This will create a document-term matrix where each entry is the count of a word in a document\n",
        "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
        "\n",
        "        # Step 2: Apply TF-IDF transformation on the bag-of-words output\n",
        "        # This will reweight the counts with Term Frequency - Inverse Document Frequency (TF-IDF),\n",
        "        # which gives more importance to rare terms and reduces the impact of frequently occurring ones.\n",
        "        (\"tfidf\", TfidfTransformer()),\n",
        "\n",
        "        # This uses the TF-IDF weighted features to train a supervised model.\n",
        "        (\"clf\", SGDClassifier(**sdg_params))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "oixJFqpiDHbL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline(\n",
        "    [\n",
        "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
        "        (\"tfidf\", TfidfTransformer()),\n",
        "        (\"clf\", SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Cbb_FpPiDbJH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LabelSpreading Pipeline\n",
        "ls_pipeline = Pipeline(\n",
        "    [\n",
        "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
        "        (\"tfidf\", TfidfTransformer()),\n",
        "        # LabelSpreading does not support dense matrices\n",
        "        (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
        "        (\"clf\", LabelSpreading()),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Fwxir9JaD6Ms"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1))\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\n",
        "        \"Micro-averaged F1 score on test set: %0.3f\"\n",
        "        % f1_score(y_test, y_pred, average=\"micro\")\n",
        "    )\n",
        "    print(\"-\" * 10)\n",
        "    print()"
      ],
      "metadata": {
        "id": "wCgXsPjhEK8H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# select a mask of 20% of the train dataset\n",
        "y_mask = np.random.rand(len(y_train)) < 0.2\n",
        "\n",
        "# X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "X_20, y_20 = map(\n",
        "    list, zip(*((x, y) for x, y, m in zip(X_train, y_train, y_mask) if m))\n",
        ")\n",
        "print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
        "eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "# set the non-masked subset to be unlabeled\n",
        "y_train[~y_mask] = -1\n",
        "print(\"SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\")\n",
        "eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n",
        "eval_and_print_metrics(ls_pipeline, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620cCFILEM0Q",
        "outputId": "3e95a451-a987-4410-e7ac-8a31f07b6b02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised SGDClassifier on 100% of the data:\n",
            "Number of training samples: 2117\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.888\n",
            "----------\n",
            "\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 445\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.766\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "Number of training samples: 2117\n",
            "Unlabeled samples in training set: 1672\n",
            "End of iteration 1, added 1080 new labels.\n",
            "End of iteration 2, added 209 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 30 new labels.\n",
            "End of iteration 5, added 18 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.827\n",
            "----------\n",
            "\n",
            "LabelSpreading on 20% of the data (rest is unlabeled):\n",
            "Number of training samples: 2117\n",
            "Unlabeled samples in training set: 1672\n",
            "Micro-averaged F1 score on test set: 0.669\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Training Pipeline (Semi-Supervised)**\n",
        "\n",
        "- **Definition**: In a self-training pipeline, the model starts with a small labeled dataset and is trained in a supervised way. After initial training, the model is applied to the unlabeled data. The model’s predictions for the unlabeled data with high confidence are then added to the training set (pseudo-labeling), and the process is repeated.\n",
        "- **Data**: Starts with a small amount of labeled data, but utilizes a large amount of unlabeled data for learning.\n",
        "- **Example Use Case**: Image classification with limited labeled examples, where the model is able to self-label additional images based on its confidence.\n",
        "  \n",
        "### **Label Spreading Pipeline (Semi-Supervised)**\n",
        "\n",
        "- **Definition**: Label spreading is a graph-based semi-supervised learning method that propagates labels through the data. It works by constructing a graph where each node represents a sample, and edges represent similarities between samples. Labeled data is used to propagate information to the unlabeled data based on their similarity.\n",
        "- **Data**: A small amount of labeled data and a large amount of unlabeled data. Labels are propagated based on feature similarity.\n",
        "- **Example Use Case**: Assigning topics to documents when only a few are labeled, but many are unlabeled, and you rely on similarities between documents to spread labels.\n",
        "  \n",
        "---\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect                  | Supervised Pipeline             | Self-Training Pipeline             | Label Spreading Pipeline          |\n",
        "|-------------------------|---------------------------------|------------------------------------|-----------------------------------|\n",
        "| **Data Requirements**    | Fully labeled dataset           | Small labeled + large unlabeled    | Small labeled + large unlabeled   |\n",
        "| **Learning Strategy**    | Purely based on labeled data    | Uses labeled + pseudo-labeled data | Spreads labels through similarity |\n",
        "| **Model Iteration**      | No iteration, standard training | Iterative: model pseudo-labels data | Spreads labels once based on graph|\n",
        "| **Model Type**           | Any standard supervised model   | Self-training classifier           | Graph-based semi-supervised model |\n",
        "| **Use Case**             | Fully labeled tasks (classification) | When labels are limited but unlabeled data is abundant | For tasks with clear similarity patterns among data points |\n",
        "\n",
        "Each approach is chosen based on how much labeled data is available and how much unlabeled data can be leveraged for learning."
      ],
      "metadata": {
        "id": "F5j1xJmmEfkI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLyFbswiFeL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}